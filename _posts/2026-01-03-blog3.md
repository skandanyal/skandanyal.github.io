---
title: Optimized KNN Classifier and Regressor  
data: 2026-01-02
--- 

# Optimizing K-th Nearest Neighbours

Compared to the previous iteration of KNN, the current models of KNN Classifiers and  
Regressors are now optimized to extract performance in the same hardware environment as 
before. 

In essense, there exist a set path in which an algorithm is built from scratch:
1. Naive implementation of the mathematical model
2. Mathematical optimizations of the same mathematical model
3. Software-centric optimizations on the model
4. Hardware-centric optimizations on the model
The KNN models were able achieve upto **200x** jump in performance compared to their 
naive implementations by following the same path.

## Mathematical optimizations 
* The `k` nearest neighbours were picked using the `std::nth_element` instead of using 
a min heap for the same.

## Software-centric optimizations
* **Row major / Column major representation:** The original 2 dimensional X matrix was originally 
represented using `std::vector<std::vector<float>>`. It was then reqritten to store 
the same elements using `std::vector<float>`. 
```
Transformation:
X[row, col] => X[row * Ncols + col]
```

## Hardware-centric optimizations
* **OpenMP parallelization:** The `train()` function was updated to support multithreading using
OpenMP. Upon profiling, using half the number of total threads (6/12) ensured the least 
training time, while ensuring a stable overhead. The compiler flag `fopenmp` is required 
to enable this functionality.
* **SIMD Vectorization:** The repeated summing of distances allowed for `reductions`, which 
led to implemting 8-bit auto-vectorization on the hardware. The `-march=native` is used 
to enable auto-vectorization. 
* **Other compiler flags:** `-O3` was used instead of `-O0` flag, which introduced hardware 
specific optimization, sometimes compromizing on mathematical stability. 
* `perf` was used to profile the code to check for code occupying high runtime.

## Result
![Results](assets/benchmarking_result_knnc.jpeg)  
When benchmarked on datasets of size varying from 500x10 to 140000x10 elements, 
* Enabled `100x` to `250x` times speed-up compared to when not using the above mentioned 
optimizations.
* Resultant code was still `4x` to `38x` slower than the `sklearn` equivalent.
* Increased the overhead code to enable the optimizations, resulting in increased compile time. 

## Future enhancements
* Loop unrolling was used, but was reverted owing to poor knowledge and proof of work.

## Learnings
* Understanding the difference between naive mathematical implementations, algorithmic implementations,
software and hardware centric optimizations and techniques.
* Importance of understanding the OS and hardware on which code runs on.
* Appreciation towards a good IDE like `CLion`.
* Concept of profiling and its uses.

Bindings or wrappers exist to make a developer's life easy. But trying to dig deeper into 
a wrapper unravels a whole new world of cog wheels, which act as the underlying structures 
behind so many popular libraries used today. Projects like these allow developers to appreciate 
the effort put into building these wrappers, directly motivating them to dig even deeper into
their own code.


