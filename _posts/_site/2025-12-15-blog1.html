<h1 id="building-my-own-c20-header-only-ml-library">Building my own C++20 header-only ML library</h1>

<p>Glacier.ML is a header-only numerical algorithms library implemented entirely in C++ with <code class="language-plaintext highlighter-rouge">Eigen</code> sometimes used for linear algebra.</p>

<p><img src="/assets/blog1/Pasted_image_1.png" alt="Glacier.ML" />
The project originated as a follow-up to formal coursework in multivariate statistical modeling, specifically linear regression and its evaluation metrics. To translate theory into implementation, I used Stanford Online’s Statistical Learning lectures as a mathematical reference, while avoiding existing ML frameworks.</p>

<p>A primitive prototype was reviewed by my AI professor, whose feedback shifted the project from experimentation to sustained development.</p>

<p>At present, Glacier.ML implements three stable models:</p>
<ul>
  <li>Simple Linear Regression</li>
  <li>Multiple Linear Regression</li>
  <li>Binary Logistic Regression</li>
</ul>

<p>The logistic regression implementation has been trained, tested, and validated on two real-world datasets:</p>
<ul>
  <li>Pima Indians Diabetes Dataset (768 × 9)</li>
  <li>Wisconsin Diagnostic Breast Cancer Dataset (569 × 32)</li>
</ul>

<p><img src="/assets/blog1/Pasted_image_2.png" alt="Confusion matrix 1" />
<img src="/assets/blog1/Pasted_image_3.png" alt="Confusion matrix 2" />
Confusion matrices for the datasets Pima Indians Diabetes Database and Wisconsin Cancer Diagnostic Dataset respectively
Press enter or click to view image in full size</p>

<p><img src="/assets/blog1/Pasted_image_4.png" alt="Comparison" />
Benchmarking training time of Glacier’s Logistic Regression against Scikit-learn’s Logistic Regression
Evaluation metrics and training-time comparisons were measured against Scikit-learn’s logistic regression. Despite lacking explicit optimization and parallelism, the results were comparable in both accuracy and training time.</p>

<p>This project exposed low-level numerical issues rarely encountered in higher-level ML workflows, including floating-point underflow and stability constraints.</p>

<p>Below is a minimal example demonstrating dataset ingestion, training, prediction, and evaluation using Glacier.ML’s binary logistic regression pipeline.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#include "Glacier/Models/MLmodel.hpp"
#include "Glacier/Utils/utilities.hpp"

int main() {
    std::vector&lt;std::vector&lt;float&gt;&gt; X, X_t;
    std::vector&lt;std::string&gt; y, y_t;

    Glacier::Utils::read_csv_c("../Datasets/training_dataset.csv", X, y, true);
    Glacier::Utils::read_csv_c("../Datasets/testing_dataset.csv", X_t, y_t, true);

    std::vector&lt;std::vector&lt;float&gt;&gt; X_p = {
        {1, 2, 3 .... n}
    };
    std::vector&lt;std::string&gt; y_p = {"label_1"};

    Glacier::Models::MLmodel md(X, y);

    float hp1 = 1.0f;
    md.train(hp1);

    auto md_pred = md.predict(X_p);
    md.analyze_2_targets(X_t, y_t);

    return 0;
}
</code></pre></div></div>
<p>This example illustrates Glacier.ML’s design goal: a minimal, explicit training and evaluation pipeline without hidden abstractions. Hyperparameters, data ingestion, and evaluation remain directly visible to the user.</p>

<p>Link to the project’s GitHub repository:</p>

<p>GitHub - https://github.com/skandanyal/Glacier.ML</p>

